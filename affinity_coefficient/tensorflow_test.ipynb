{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weights:0' shape=(4, 3) dtype=float32_ref>\n",
      "<tf.Variable 'biases:0' shape=(3,) dtype=float32_ref>\n",
      "Tensor(\"Sigmoid:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_input = len(iris.data[0])\n",
    "n_output = 3 # [0,1,2]... set(iris.target)\n",
    "\n",
    "input_shape = [None,n_input]\n",
    "inputplaceholder = tf.placeholder(dtype=tf.float32, shape=input_shape, name=\"input_placeholder\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_output]), name=\"weights\")\n",
    "biases = tf.Variable(tf.zeros([n_output]), name=\"biases\")\n",
    "\n",
    "print(weights)\n",
    "print(biases)\n",
    "\n",
    "layer_1 = tf.matmul(inputplaceholder, weights)\n",
    "layer_2 = tf.add(layer_1, biases)\n",
    "outputlayer = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "print(outputlayer)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "labelsplaceholder = tf.placeholder(dtype=tf.float32, shape=[None,n_output], name=\"labels_placeholder\")\n",
    "#cost = tf.losses.mean_squared_error(labelsplaceholder, outputlayer)\n",
    "cross_entropy_cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=labelsplaceholder,logits=outputlayer)\n",
    "cost = tf.reduce_mean(cross_entropy_cost)\n",
    "\n",
    "print(cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "batch_size = 10\n",
    "iterations = 400\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(iris.data)\n",
    "scaled_data = scaler.transform(iris.data)\n",
    "mydata = list(zip(scaled_data, iris.target))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "history_loss = list()\n",
    "for _ in range(iterations):\n",
    "    inputdata = list()\n",
    "    output_data = list()\n",
    "    for _ in range(batch_size):\n",
    "        input_output_pairs = random.choice(mydata)\n",
    "        inputdata.append(input_output_pairs[0])\n",
    "        output_one_hot = [0.0,0.0,0.0]\n",
    "        output_one_hot[input_output_pairs[1]] = 1.0\n",
    "        output_data.append(output_one_hot)\n",
    "\n",
    "    feed_dict={inputplaceholder: inputdata, labelsplaceholder: output_data}\n",
    "    res_optimizer, res_cost = sess.run([optimizer, cost], feed_dict=feed_dict)\n",
    "    #print(res_cost)\n",
    "    history_loss.append(res_cost)\n",
    "    \n",
    "    #stampa il dettaglio per debug \n",
    "    #print(labelsplaceholder.eval(feed_dict, sess))\n",
    "    #print(outputlayer.eval(feed_dict, sess))\n",
    "    #print(cross_entropy_cost.eval(feed_dict, sess))\n",
    "    #print(cost.eval(feed_dict, sess))\n",
    "    #print(\"---------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "logit: \n",
      "[[0.35434368 0.6456563 ]\n",
      " [0.59868765 0.40131232]\n",
      " [0.450166   0.549834  ]]\n",
      "\n",
      "sigmoid_cross_entropy_with_logits: \n",
      "[[0.79813886 0.3711007 ]\n",
      " [0.40318602 0.8543552 ]\n",
      " [0.5130153  1.037488  ]]\n",
      "\n",
      "sigmoid_cross_entropy_with_logits a mano: \n",
      "[[0.7981388  0.37110066]\n",
      " [0.40318608 0.8543552 ]\n",
      " [0.5130153  1.0374879 ]]\n",
      "\n",
      "sigmoid_cross_entropy_with_logits + reduce_mean: \n",
      "0.66288066\n",
      "\n",
      "sigmoid_cross_entropy_with_logits + reduce_sum: \n",
      "3.977284\n",
      "\n",
      "softmax_cross_entropy_with_logits: \n",
      "[0.437488   0.5130153  0.79813886]\n",
      "\n",
      "softmax_cross_entropy_with_logits a mano: \n",
      "[0.43748796 0.5130153  0.7981389 ]\n",
      "\n",
      "softmax_cross_entropy_with_logits + reduce_mean: \n",
      "0.5828807\n",
      "\n",
      "softmax_cross_entropy_with_logits + reduce_sum: \n",
      "1.7486421\n",
      "\n",
      "mean_squared_error: \n",
      "0.19630949\n",
      "\n",
      "mean_squared_error a mano: \n",
      "[[0.12555945 0.12555946]\n",
      " [0.1610516  0.16105159]\n",
      " [0.30231744 0.30231744]]\n",
      "0.19630949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#esempio minimale di esempio\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "p = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "logit_q = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "q = tf.nn.softmax(logit_q)\n",
    "\n",
    "feed_dict = {\n",
    "  p: [[0, 1],\n",
    "      [1, 0],\n",
    "      [1, 0]],\n",
    "  logit_q: [[0.2, 0.8],\n",
    "            [0.7, 0.3],\n",
    "            [0.4, 0.6]]\n",
    "}\n",
    "\n",
    "prob_sigmoid_crossentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q)\n",
    "prob_sigmoid_crossentropy_homemade = p * -tf.log(tf.sigmoid(logit_q)) + (1-p) * -tf.log(1-tf.sigmoid(logit_q))\n",
    "prob_sigmoid_crossentropy_mean = tf.reduce_mean(prob_sigmoid_crossentropy)\n",
    "prob_sigmoid_crossentropy_sum = tf.reduce_sum(prob_sigmoid_crossentropy)\n",
    "prob_softmax_crossentropy = tf.nn.softmax_cross_entropy_with_logits(labels=p, logits=logit_q)\n",
    "prob_softmax_cross_entropy_with_logits_handmade = -tf.reduce_sum(p * tf.log(q), axis=1)\n",
    "prob_softmax_crossentropy_mean = tf.reduce_mean(prob_softmax_crossentropy)\n",
    "prob_softmax_crossentropy_sum = tf.reduce_sum(prob_softmax_crossentropy)\n",
    "prob_mean_squared_error = tf.losses.mean_squared_error(p, q)\n",
    "prob_mean_squared_error_squared_difference_homemade = tf.squared_difference(q, p)\n",
    "prob_mean_squared_error_squared_difference_mean_homemade = tf.reduce_mean(tf.squared_difference(q, p)) \n",
    "print(\"target: \")\n",
    "print(p.eval(feed_dict))\n",
    "print()\n",
    "print(\"logit: \")\n",
    "print(q.eval(feed_dict))\n",
    "print()\n",
    "print(\"sigmoid_cross_entropy_with_logits: \")\n",
    "print(prob_sigmoid_crossentropy.eval(feed_dict))\n",
    "print()\n",
    "print(\"sigmoid_cross_entropy_with_logits a mano: \")\n",
    "print(prob_sigmoid_crossentropy_homemade.eval(feed_dict))\n",
    "print()\n",
    "print(\"sigmoid_cross_entropy_with_logits + reduce_mean: \")\n",
    "print(prob_sigmoid_crossentropy_mean.eval(feed_dict))\n",
    "print()\n",
    "print(\"sigmoid_cross_entropy_with_logits + reduce_sum: \")\n",
    "print(prob_sigmoid_crossentropy_sum.eval(feed_dict))\n",
    "print()\n",
    "print(\"softmax_cross_entropy_with_logits: \")\n",
    "print(prob_softmax_crossentropy.eval(feed_dict))\n",
    "print()\n",
    "print(\"softmax_cross_entropy_with_logits a mano: \")\n",
    "print(prob_softmax_cross_entropy_with_logits_handmade.eval(feed_dict))\n",
    "print()\n",
    "print(\"softmax_cross_entropy_with_logits + reduce_mean: \")\n",
    "print(prob_softmax_crossentropy_mean.eval(feed_dict))\n",
    "print()\n",
    "print(\"softmax_cross_entropy_with_logits + reduce_sum: \")\n",
    "print(prob_softmax_crossentropy_sum.eval(feed_dict))\n",
    "print()\n",
    "print(\"mean_squared_error: \")\n",
    "print(prob_mean_squared_error.eval(feed_dict))\n",
    "print()\n",
    "print(\"mean_squared_error a mano: \")\n",
    "print(prob_mean_squared_error_squared_difference_homemade.eval(feed_dict))\n",
    "print(prob_mean_squared_error_squared_difference_mean_homemade.eval(feed_dict))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
